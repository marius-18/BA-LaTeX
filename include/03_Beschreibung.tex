Wie bereits in Kapitel \ref{sec:global_curveball} erwähnt muss die disjunkte Nachbarschaft
der beiden Knoten $u$ und $v$ bekannt sein, um einen \ct{} auf diesen Knoten $u$ und $v$
auszuführen. Der \ct{} besteht dann darin, diese Knoten aus $N_{d}(u,v)$ zu durchmischen.
Deshalb beschäftigen wir uns zuerst damit, wie man die disjunkte Nachbarschaft bestimmt.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% COMMON NEIGHBORS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Bestimmung der disjunkten Nachbarschaft}
\label{sec:common}
Gesucht sind alle Knoten aus der disjunkten Nachbarschaft der Knoten $u$ und $v$.
Nach der Definition \ref{def:common_disjoint} liegt jeder Knoten aus den Nachbarschaften $N(u)$ und $N(v)$ 
entweder in der disjunkten oder in der gemeinsamen Nachbarschaft. Das Ziel besteht also darin, 
für jeden Knoten aus $N(u) \cup N(v)$ zu entscheiden, ob er zu $N_{d}(u,v)$ oder $N_{c}(u,v)$ gehört.
\\
Die Nachbarschaften $N(u)$ und $N(v)$ liegen wie in Abschnitt \ref{sec:datenstruktur} beschrieben jeweils
in einem Array vor. Der Übersichtlichkeit wegen, werden wir die beiden
Arrays ebenfalls mit $N(u)$ und $N(v)$ bezeichnen. Die Aufgabe ist es also,
 für jedes Element aus den beiden Arrays zu entscheiden,
ob es entweder in beiden Arrays vorkommt, oder nur in einem von den beiden. Dafür 
gibt es verschiedene Möglichkeiten, dies umzusetzen.
\\
\\
Als ersten naiven Ansatz könnte man für jedes Element des Arrays $N(u)$ das gesamte andere 
Array $N(v)$ per linearer Suche nach diesem Element durchsuchen. Hierfür ergibt sich eine Laufzeit von
$\O(|N(u)|\cdot|N(v)|)$. Dies ist aber natürlich nicht sinnvoll ist, da das Array $N(v)$ ziemlich oft 
durchlaufen werden muss und wir 
im Falle von massiven Graphen davon ausgehen können, dass die Arrays (also die Nachbarschaften)
 groß werden. 
\\
Dieses Problem kann man beispielsweise verhindern, indem man beide Arrays aufsteigend sortiert. 
Um nun zu herauszufinden,
welche Werte in beiden Arrays vorkommen, muss man lediglich $u$ und $v$ gleichzeitig linear durchlaufen
und testen, ob die Werte gleich sind, oder nicht. Somit muss man jedes Element der beiden Arrays -- nach dem Sortieren -- 
nur einmal betrachten, was offensichtlich zu einer verbesserten Laufzeit im Vergleich zum naiven
Ansatz führt. Man erhält damit eine Laufzeit von $\O(|N(u)|\cdot \log (|N(u)|)  + |N(v)|\cdot\log(|N(v)|))$. 
Diese Variante wird im Folgenden als \SorSor{} bezeichnet. 
\\
Die Laufzeit hängt dabei im Wesentlichen vom Sortieren ab. Daher führen wir eine Invariante ein, 
die wir \fett{vorsortiert} nennen. Bei dieser Invariante nehmen wir an, dass
die Arrays immer im sortierten Zustand vorliegen. Somit würde bei \SorSor{} das Sortieren wegfallen
und man müsste die beiden Arrays nur noch linear durchlaufen, was zu einer Laufzeit von $\O(|N(u)| + |N(v)|)$
führen würde. 
Es ist jedoch nicht offensichtlich, dass die Invariante zu einer insgesamt besseren Laufzeit eines
\ct{es} führt, da ein \ct{} schließlich auch noch aus dem Durchmischen der disjunkten Nachbarschaft besteht.
Dadurch könnte die Invariante verletzt werden, weshalb man am Ende des \ct{es} nochmal sortieren
muss, um sie wieder aufrecht zu erhalten. Ob die Invariante zu einem Vorteil führt, wird im Kapitel 
\ref{cap:tests} anhand von Laufzeitmessungen geklärt.
\\
\\
Die Variante \SorSor{} lässt sich leicht abwandeln, indem wir nur eines der beiden Arrays sortieren.
Auf diese Weise können wir die Laufzeit des einen Sortiervorgangs sparen. Sortieren
wir das größere Array (ohne Beschränkung der Allgemeinheit $N(u)$), bezeichnen wir die Variante als \SorSea{}. Um zu erkennen, 
welche Elemente zur gemeinsamen und welche zur disjunkten Nachbarschaft gehören, kann man für jedes
Knoten aus $N(v)$ per binärer Suche in logarithmischer Zeit prüfen, ob der Knoten auch in $N(u)$ vorhanden ist, oder nicht.
Damit ergibt sich eine Laufzeit von $\O(|N(u)| \cdot \log(|N(u)|) + |N(v)| \cdot \log(|N(u)|)$. Analog dazu
nennen wir die Variante, in der das kleinere Array sortiert wird, \SeaSor. 
Auch hier könnte die vorsortiert Invariante einen Vorteil bringen.
\\
Eine weitere Methode um viele Werte schnell zu durchsuchen, bietet die Datenstruktur \texttt{set}, welche
eines binären Suchbaums entspricht, beispielsweise eines Rot-Schwarz-Baums.
Dabei wird jedes Element des einen Arrays in den Suchbaum eingefügt. 
Für jedes Element des anderen Arrays kann nun in logarithmischer
Zeit bestimmt werden, ob es im Set und somit auch im ursprünglichen Array vorhanden ist.
\\ 
\red{Ein Vorteil dieser Variante könnte darin liegen...? amortisierte laufzeiten? }
\\
Für diese Möglichkeit gibt es ebenfalls zwei 
analoge Varianten, nämlich \SetSea, bei der die Elemente des größeren Arrays in das Set eingefügt werden
und \SeaSet, bei der das kleinere Array zum Set hinzugefügt wird.
Auch bei den beiden Optionen kann es sinnvoll sein, wenn die vorsortiert Invariante aufrecht erhalten wird. Je
nachdem, wie die interne Implementierung des Suchbaums aussieht, könnte es einen Laufzeitvorteil beim Einfügen
der Werte geben, wenn
diese bereits sortiert sind.
\\
Die letzte Methode, die wir an dieser Stelle betrachten werden,
ist die Verwendung der Datenstruktur \textit{unordered\_set}. Diese ist sehr ähnlich wie Set, mit
dem Unterschied, dass die Werte nicht in geordneter Reihenfolge gespeichert werden, sondern
in einer Hash-Tabelle. Ein Vorteil einer Hash-Tabelle liegt darin, dass das Suchen von Elementen
\red{erwartet in konstanter} Zeit erfolgt.
Ebenfalls gibt es hierbei wieder die Varianten, 
in denen das größere Array in das unordered\_set eingefügt wird (\USetSea) 
oder das kleinere (\SeaUSet).{}
Schließlich werden wir auch bei den letzten beiden Methoden prüfen,
 ob die Invariante eventuell zu einer besseren Laufzeit führt.
\\
\\
Zusammenfassend betrachten wir also insgesamt sieben verschiedene Möglichkeiten um die disjunkte
und gemeinsame Nachbarschaft zu berechnen. Für jede dieser Varianten prüfen wir zusätzlich,
ob die vorsortiert Invariante zu einem Laufzeittechnischen Vorteil führt oder ob es sich
nicht lohnt, diese aufrechtzuerhalten.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% Nachbarn Tauschen
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Tauschen der Nachbarn}
\label{sec:trade}
Im vorherigen Teil wurde beschrieben, wie man die gemeinsame und die disjunkte Nachbarschaft zweier Knoten
$u$ und $v$ bestimmt. Nun beschäftigen wir uns damit, wie man die Knoten der disjunkten Nachbarschaft zufällig tauscht. Als Eingabe 
stehen die Arrays $N_{d}(u,v)$, welches alle Knoten aus der disjunkten Nachbarschaft enthält
und $N_{c}(u,v)$, das die gemeinsamen Nachbarn enthält, zur Verfügung. Weiterhin seien $\text{deg}(u)$ und
$\text{deg}(v)$ die ursprünglichen Knotengrade. Zusätzlich definieren wir uns die Mengen
$D_{u} = N_{d}(u,v) \cap N(u)$, in der die disjunkten Nachbarn von Knoten $u$ liegen, 
und $D_{v} = N_{d}(u,v) \cap N(v)$, in der die disjunkten Nachbarn von $v$ liegen. Die anfänglich
leeren Arrays, in denen die Ausgabe -- also die neuen Nachbarschaften von $u$ und $v$ -- zurückgegeben 
werden soll, bezeichnen wir wieder als $N(u)$ und $N(v)$.
Wir betrachten zwei unterchiedliche Möglichkeiten.
\\
\\
Die erste Idee besteht darin,  
das Array der disjunkten Nachbarschaft zufällig zu permutieren, sodass jedes Element an 
einer zufälligen Position steht. Um nun die beide \glqq neuen\grqq{} Nachbarschaften von $u$ und $v$ zu erstellen,
werden zuerst die Knoten aus der gemeinsamen Nachbarschaft in die leeren Arrays $N(u)$ und $N(v)$ kopiert.
Dann werden die ersten $D_{u}$ Elemente aus dem permutierten Array in $N(u)$ kopiert, die restlichen
in $N(v)$. Somit haben die Nachbarschaften durch den Tausch ihre ursprüngliche Größe nicht verändert, es gilt $|N(u)| = \deg(u)$ und
$|N(v)| = \deg(v)$.
Zur besseren Veranschaulichung ist in Abbildung \ref{fig:trade_shuffle} ein Beispiel zu sehen.
\begin{figure}
\centering
  \begin{tikzpicture}[decoration=brace]
    % Die Grundlinie:
    \draw(0,0)--(10,0);
    \draw(0,1)--(10,1);

    % Striche und Beschriftung in Abständen 0, 2, 4, 6, ...
    \foreach \x/\xtext in {0,1,2,3,4,5,6,7,8,9,10}
      \draw(\x,0)--(\x,1) node[below] {};
      
    % untere geschweifte Klammer mit Text darunter:
    \draw[decorate, yshift=-1ex] (3.8,0) -- node[below=0.4ex] {$D_{u}$} (0.2,0);

    \draw[decorate, yshift=-1ex] (9.8,0) -- node[below=0.4ex] {$D_{v}$} (4.2,0);

\node[] at (-1.2, 0.5)     (5)     {$N_{d}(u,v):$};

  \end{tikzpicture}
  \caption{Beispiel für einen Tausch mit der Variante \perm. Dabei wird das Array der 
  disjunkten Nachbarschaft $N_{d}(u,v)$ zufällig permutiert. Die ersten $D_{u}$ Elemente werden der Nachbarschaft
  von $u$ zugeordnet, die restlichen $D_{v}$ zur Nachbarschaft von $v$. }
  \label{fig:trade_shuffle}
\end{figure}
Bei die diesem Verfahren fällt  jedoch auf, dass einige Elemente beim Permutieren unnötig vertauscht werden.
Für jedes Element ist es eigentlich nur entscheidend, ob es unter den ersten $D_{u}$  liegt (also zum Array
$N(u)$ hinzugefügt wird) oder nicht. Auf welcher Position genau es in 
diesen Bereichen liegt, ist nicht von Relevanz. Ohne Beschränkung der Allgemeinheit gelte 
$N(u) \le N(v)$. Dann kann man
also die Laufzeit dieser Variante verbessern, indem nicht das ganze Array der disjunkten Nachbarschaft zufällig permutiert 
wird, sondern nur die ersten $D_{u}$ \red{vielen} Elemente zufällig gewählt werden.
 Dies setzt die Funktion 
\textit{random\_bipartition\_shuffle} um.
\\
Ein Nachteil dieser Methode besteht jedoch darin, dass durch das zufällige Vertauschen die beiden resultierenden Arrays
$N(u)$ und $N(v)$ nicht mehr unbedingt sortiert sind. Damit wird die im Abschnitt \ref{sec:common} beschriebene
Invariante eventuell verletzt. Möchte man die Invariante aufrecht erhalten, müssen somit die beiden Arrays
in einem letzten Schritt nochmals sortiert werden.
Wir nennen diese Variante \perm.
\\
\\
Die zweite Möglichkeit, die wir betrachten, werden wir als \distr{} bezeichnen.
\\
Die Idee besteht dabei, dass wir über jedes Element des Arrays $N_{d}(u,v)$ iterieren und eine Wahrscheinlichkeit
berechnen, mit
der das Element in die Nachbarschaft von $u$ (beziehungsweise $v$) eingefügt werden soll. Dann wird in einem 
Bernoulli Experiment mit genau dieser Wahrscheinlichkeit ein Zufallsbit gezogen. Je nachdem, welchen
Wert das Zufallsbit hat, wird das Element dann entweder in $N(u)$ oder in $N(v)$ kopiert. Dies wird so lange
wiederholt, bis eines der beiden Arrays seine maximale Kapazität erreicht hat. 
\\
Um die Wahrscheinlichkeit zu berechnen werden am Anfang zwei Variablen $n_v$ und $n_u$ initialisiert, 
welche den Kapazitäten der beiden Arrays $u$ und $v$ entsprechen, wenn die Elemente aus der 
gemeinsamen Nachbarschaft nicht berücksichtigt werden. \red{Es gilt also $n_v = |D_{v}|$ und
$n_u= |D_{u}|$.}
Damit hat das erste Element des Arrays $N_{d}(u,v)$ eine Wahrscheinlichkeit von $p_u = \frac{n_u}{n_u+n_v}$, dem
Array $N(u)$ hinzugefügt zu werden und analog eine Wahrscheinlichkeit $p_v = \frac{n_v}{n_u+n_v}$, um
in $N(v)$ zu gelangen. Offensichtlich gilt $p_u + p_v = 1$. Dann wird mit einer der beiden
Wahrscheinlichkeiten das Bernoulli Experiment durchgeführt, wobei es egal ist, welche Wahrscheinlichkeit
man dazu wählt, da $p_u$ genau die Gegenwahrscheinlichkeit von $p_v$ ist und umgekehrt. 
Wählt man beispielsweise $p_u$ und das Experiment liefert eine eins, dann wird das aktuelle Element
in $N(u)$ kopiert. Dabei
hat sich aber offensichtlich die verbleibende Kapazität des Arrays $N(u)$ verringert. Also muss
der Wert $n_u$ dekrementiert werden. Analoges gilt, falls das Element in die Nachbarschaft von $v$ kopiert wird.
Somit ändern sich nach jeder Iteration die Wahrscheinlichkeiten $p_u$ beziehungsweise $p_v$.
Gilt nach irgendeinem Zeitpunkt entweder $n_u = 0$ oder $n_v = 0$, steht offenbar in einem der Arrays 
keine freie Kapazität mehr zur Verfügung. Somit werden die übrigen Elemente, die noch in $N_{d}(u,v)$ vorhanden sind, 
einfach dem anderen Array hinzugefügt.
\\
\red{Dieses Vorgehen ist auch als Reservoir Sampling\cite{...?} wo? bekannt.}
\\
Ein Vorteil dieser Methode ist, dass die in \ref{sec:common} beschriebene Invariante aufrecht erhalten werden kann.
War das Array der disjunkten Nachbarschaft vor Beginn dieser Methode aufsteigend sortiert,
dann sind auch die bisherigen Elemente der Arrays $N(u)$ und $N(v)$ aufsteigend sortiert, da für jedes
Element nacheinander entschieden wurde, ob es zur Nachbarschaft von $u$ oder von $v$ hinzugefügt wird und dabei die
Reihenfolge der Elemente untereinander nicht verändert wurde.
\\
Zum Schluss müssen noch die die gemeinsamen Nachbarn zu den Arrays $N(u)$ und $N(v)$ hinzugefügt werden.
Möchte man die Invariante aufrecht erhalten, dann sind die beiden Arrays wie beschrieben schon
aufsteigend sortiert. Da auch die Elemente aus $N_{c}(u,v)$ aufsteigend sortiert sind, erhält man
die endgültigen Arrays von $N(u)$ und $N(v)$ durch ein Mergen mit $N_{c}(u,v)$. Soll die Invariante jedoch
nicht aufrecht erhalten werden, reicht es aus, die Elemente aus $N_{c}(u,v)$ jeweils an das Ende der beiden Arrays
zu kopieren.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% Global Curveball Tausch
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{\ct}
Wie schon in Abschnitt \ref{sec:global_curveball} beschrieben, 
besteht ein Curveball Tausch auf zwei Knoten $u$ und $v$ daraus, die 
gemeinsame und disjunkte Nachbarschaft der beiden Vektoren zu bestimmen
und schließlich die Knoten aus der disjunkten Nachbarschaft zufällig zu tauschen.
\\
Die verschiedenen Methoden die wir hierfür untersuchen werden 
entstehen durch Kombination aller Varianten, die in \ref{sec:common} und \ref{sec:trade} beschrieben 
werden. Alle diese Möglichkeiten sind in der Tabelle \ref{tab:varianten} zusammengefasst.


\begin{table}
	\centering
	\begin{tabular}{c||c|c||c|c}
		 & \multicolumn{2}{c||}{\distr} & \multicolumn{2}{c}{\perm} \\
		 & \true & \false & \true & \false
		\\ \hline\hline
		\SorSor & & & & \\ \hline\hline
		\SeaSor & & & &\\ \hline\hline
		\SorSea & & & &\\ \hline\hline
		\SeaSet & & & &\\ \hline\hline
		\SetSea & & & &\\ \hline\hline
		\SeaUSet & & & &\\ \hline\hline
		\USetSea& & & &
	\end{tabular}
	\caption{Jedes Feld in der Tabelle entspricht einer Variante für einen \ct{}. Die Werte true 
	und false stehen jeweils dafür, ob die vorsortiert Invariante genutzt wird oder nicht.}
	\label{tab:varianten}
\end{table}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% Ṕseudocode
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Pseudocode}
Wie wir in Kapitel \ref{sec:entscheidung} sehen werden, 
hat die Variante mit den Methoden \SorSor{} und \distr{} und der genutzten vorsortiert
Invariante das beste Laufzeitverhalten. Deswegen gehen wir an dieser Stelle 
noch einmal genauer auf diese beiden Methoden ein, indem wir sie in Pseudocode
beschreiben. In Algorithmus \ref{algo:sortsort} ist \SorSor{} beschrieben.
\red{...noch was dazu schreiben...?}

\begin{algorithm}
  \caption{SortSort}\label{algo:sortsort}
  \begin{algorithmic}[1]
    \Procedure{SortSort}{$u,v$}
	  \State U $ \gets N(u)$ \Comment{vorsortierte Nachbarschaft von u}
	  \State V $ \gets N(v)$ \Comment{vorsortierte Nachbarschaft von v}
	  %\State
	  \State nu $\gets 0$ \Comment{Zähler für U}
	  \State nv $\gets 0$ \Comment{Zähler für V}
	  %\State disjoint $ \gets \emptyset$
	  %\State common $ \gets \emptyset$
      
      \While{(nu < U.\texttt{size()}) and (nv < V.\texttt{size()})}
        \If{U[nu] < V[nv]}
			\State disjoint.\texttt{append(}U[nu]\texttt{)} \Comment{Füge das Element in disjoint ein}
			\State nu ++
			\ElsIf{U[nu] > V[nv]}
				\State disjoint.\texttt{append(}V[nv]\texttt{)} \Comment{Füge das Element in disjoint ein}
				\State nv ++
			\ElsIf{U[nu] == V[nv]}
				\State common.\texttt{append(}U[nu]\texttt{)} \Comment{Füge das Element in common ein}
				\State nu ++
				\State nv ++
        \EndIf
      \EndWhile
      \If{nu $\neq$ U.\texttt{size()}} \Comment{Die restlichen Elemente sind disjunkte Nachbarn}
			\State disjoint.\texttt{append(} U[nu], U[nu+1], \dots\texttt{)}
			\Else{}
			\State disjoint.\texttt{append(} V[nv], V[nv+1], \dots\texttt{)}
      \EndIf
      \State \textbf{return} common, disjoint
   \EndProcedure
  \end{algorithmic}
  \label{algo:sortsort}
\end{algorithm}



\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{Distribution}{common, disjoint}
	\State nu $\gets$ U.\texttt{size()} - common.\texttt{size()} \Comment{Kapazität von $u$}
	\State nv $\gets$ V.\texttt{size()} - common.\texttt{size()} \Comment{Kapazität von $v$}
	\State i = 0
	\While{i < disjoint.\texttt{size()}}
		\State X $\sim \mathcal{B}(\frac{\text{nu}}{\text{nu}+\text{nv}})$  \Comment{ziehe ein Zufallsbit Bernoulli verteilt mit Wahrscheinlichkeit $\frac{nu}{nu+nv}$}

		\If{X==1}
			\State U.\texttt{append(}disjoint[i]\texttt{)} \Comment{Füge das Element in U ein}
			\State i++
			\State nu- - \Comment{aktualisiere die Kapazität}
		\Else{} 
			\State V.\texttt{append(}disjoint[i]\texttt{)}\Comment{Füge das Element in V ein}
			\State i++
			\State nv- -\Comment{aktualisiere die Kapazität}
		\EndIf{}
	\EndWhile{}	
	\State U.\texttt{merge(}common\texttt{)} \Comment{Merge U mit der gemeinsame Nachbarschaft}
	\State V.\texttt{merge(}common\texttt{)} \Comment{Merge V mit der gemeinsame Nachbarschaft}
	\State \textbf{return} U, V
\EndProcedure
\end{algorithmic}
	\caption{Distribution}
	\label{algo:distr}
\end{algorithm}





%\red{bei vorsortiert fällt Zeile \ref{algo:inv1} und \ref{algo:inv2} raus!!}
%\\
%\\
%\red{Distribution}


